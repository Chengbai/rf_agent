{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.config import Config\n",
    "from src.episode import Episode\n",
    "from src.episode_dataset import EpisodeRLDataset\n",
    "from src.rl_data_record import RLDataRecord\n",
    "from src.policy_factory import PolicyMode, PolicyFactory\n",
    "from src.reward_model import RewardModel\n",
    "from src.grpo_trainer import GRPOTrainer\n",
    "from src.policy_model_utils import load_policy_model, save_policy_model, train_and_plot_policy, inference_and_plot_policy_v2\n",
    "from src.utils import get_color, normalize_min_max, to_device_collate, top_k_sampling\n",
    "from src.episode_batch_repeat_sampler import EpisodeBatchRepeatSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d5dac",
   "metadata": {},
   "source": [
    "# Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30afabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "reward_model = RewardModel(config=config)\n",
    "test_policy = PolicyFactory.create(\n",
    "    policy_mode=PolicyMode.TRANSFORMER_WITH_LATE_POSITION_FUSION, config=config\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "\n",
    "train_dataset = EpisodeRLDataset(config=config, split=\"TRAIN\")\n",
    "print(f\"train_dataset : {len(train_dataset)}\")\n",
    "\n",
    "test_dataset = EpisodeRLDataset(config=config, split=\"TEST\")\n",
    "print(f\"test_dataset : {len(test_dataset)}\")\n",
    "\n",
    "eval_dataset = EpisodeRLDataset(config=config, split=\"EVAL\")\n",
    "print(f\"eval_dataset : {len(eval_dataset)}\")\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "def get_data_loader(\n",
    "    dataset: EpisodeRLDataset, batch_size: int, group_size: int, repeats: int\n",
    "):\n",
    "    batch_repeat_sampler = EpisodeBatchRepeatSampler(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        group_size=group_size,\n",
    "        repeats=repeats,\n",
    "    )\n",
    "    # print(\n",
    "    #     f\"batch_repeat_sampler: {len(batch_repeat_sampler)},  {list(batch_repeat_sampler)}\"\n",
    "    # )\n",
    "    # print(\n",
    "    #     f\"batch ids: {[e.episode_id for e in dataset.get_episods(batch_episode_indices=list(batch_repeat_sampler))]}\"\n",
    "    # )\n",
    "\n",
    "    to_device_collate_configurable = partial(to_device_collate, config.device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size * group_size,\n",
    "        sampler=batch_repeat_sampler,\n",
    "        collate_fn=to_device_collate_configurable,\n",
    "    )\n",
    "    print(f\"data loader: {dataset.split}, {len(dataloader)}\")\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = get_data_loader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config.train_batch_size,\n",
    "    group_size=config.episode_group_size,\n",
    "    repeats=config.episode_steps,\n",
    ")\n",
    "\n",
    "test_dataloader = get_data_loader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=config.test_batch_size,\n",
    "    group_size=config.episode_group_size,\n",
    "    repeats=config.episode_steps,\n",
    ")\n",
    "\n",
    "eval_dataloader = get_data_loader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=config.eval_batch_size,\n",
    "    group_size=config.episode_group_size,\n",
    "    repeats=config.episode_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_samples = 4\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=episode_samples, ncols=3, squeeze=False, figsize=(15, 20)\n",
    ")\n",
    "\n",
    "for eidx in range(episode_samples):\n",
    "    es = train_dataset.get_episode(eidx * config.episode_group_size)\n",
    "    es.viz(ax=axes[eidx][0], reward_model=reward_model)\n",
    "\n",
    "    # Viz fov\n",
    "    fov = es.fov(center_pos=es.agent.start_state.position())\n",
    "    # print(f\"fov: {fov.size()}, {fov}\")\n",
    "    # print(f\"fov: {fov}\")\n",
    "    es.viz_fov(ax=axes[eidx][1])\n",
    "    es.viz_optimal_path(ax=axes[eidx][2])\n",
    "\n",
    "    es = train_dataset.get_episode(eidx)\n",
    "    print(f\"best_path: {es.best_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3306c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b99959",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_samples = 4\n",
    "fig, axes = plt.subplots(nrows=episode_samples, ncols=2, figsize=(10, 20))\n",
    "\n",
    "for eidx in range(episode_samples):\n",
    "    es = train_dataset.get_episode(eidx)\n",
    "    es.optimal_path()\n",
    "\n",
    "    es.viz(ax=axes[eidx][0], reward_model=reward_model)\n",
    "\n",
    "    # Viz fov\n",
    "    fov = es.fov(center_pos=es.agent.start_state.position())\n",
    "    # print(f\"fov: {fov.size()}, {fov}\")\n",
    "    # print(f\"fov: {fov}\")\n",
    "    es.viz_fov(ax=axes[eidx][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba454e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb1f495",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d1f5972",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyFactory.create(\n",
    "    policy_mode=PolicyMode.TRANSFORMER_WITH_LATE_POSITION_FUSION, config=config\n",
    ").to(config.device)\n",
    "reward_model = RewardModel(config=config)\n",
    "trainer = GRPOTrainer(config=config, policy=policy, reward_model=reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f64fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.policy.img_to_emb.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_and_plot_policy_v2(\n",
    "    config=config,\n",
    "    dataset=test_dataset,\n",
    "    dataloader=test_dataloader,\n",
    "    policy=policy,\n",
    "    reward_model=reward_model,\n",
    "    top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1299b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ioff()\n",
    "trainer.run(train_dataset=train_dataset, eval_dataset=eval_dataset, debug=False)\n",
    "save_policy_model(policy=policy)\n",
    "\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8650bc5",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e825da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_and_plot_policy_v2(\n",
    "    config=config,\n",
    "    dataset=test_dataset,\n",
    "    dataloader=test_dataloader,\n",
    "    policy=policy,\n",
    "    reward_model=reward_model,\n",
    "    top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b82345",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_and_plot_policy_v2(\n",
    "    config=config,\n",
    "    dataset=test_dataset,\n",
    "    dataloader=test_dataloader,\n",
    "    policy=policy,\n",
    "    reward_model=reward_model,\n",
    "    top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a462e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_model_path = (\n",
    "#     \"/Users/chengbai/ml/cheng_git/rf_agent/rf_model_policy_20250510230738_base.pt\"\n",
    "# )\n",
    "# trained_policy = load_policy_model(config=config, policy_model_path=policy_model_path)\n",
    "# assert trained_policy is not None\n",
    "\n",
    "# episode = inference_and_plot_policy(\n",
    "#     policy=trained_policy, config=config, reward_model=reward_model, steps=5, debug=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
